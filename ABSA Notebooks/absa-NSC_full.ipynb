{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABSA on Preds Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install clean-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load libraries\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains import SequentialChain\n",
    "import openai\n",
    "from getpass import getpass\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import cleantext\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "nsc_posts = pd.read_csv('../Reddit Data/nsc_posts_clean.csv')\n",
    "nsc_comments = pd.read_csv('../Reddit Data/nsc_comments_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load helper functions\n",
    "class Post:\n",
    "    def __init__(self, post_id, title, content):\n",
    "        self.post_id = post_id\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.comments = []\n",
    "\n",
    "class Comment:\n",
    "    def __init__(self, comment_id, text, post_id):\n",
    "        self.comment_id = comment_id\n",
    "        self.text = text\n",
    "        self.post_id = post_id  # Store the post_id\n",
    "        self.parent_comment = None\n",
    "        self.replies = []\n",
    "\n",
    "# Create dictionaries to map post IDs to Post objects and comment IDs to Comment objects.\n",
    "posts_dict = {}\n",
    "comments_dict = {}\n",
    "\n",
    "# Populate posts_dict and comments_dict from your dataframes.\n",
    "for post_row in nsc_posts.itertuples():\n",
    "    post_id = post_row.id\n",
    "    title = post_row.Title\n",
    "    content = post_row.Content\n",
    "    post = Post(post_id, title, content)\n",
    "    posts_dict[post_id] = post\n",
    "\n",
    "\n",
    "comments_df = nsc_comments.rename(columns={\n",
    "    'Comment ID': 'Comment_ID',\n",
    "    'Parent Comment ID': 'Parent_Comment_ID',\n",
    "    'Text': 'Text',\n",
    "    'Author': 'Author',\n",
    "    'Date': 'Date',\n",
    "    'Post ID': 'Post_ID'\n",
    "})\n",
    "\n",
    "# Now the columns have underscores instead of spaces, making it easier to access them.\n",
    "\n",
    "# You can use the updated column names directly in your code as follows:\n",
    "for comment_row in comments_df.itertuples():\n",
    "    comment_id = comment_row.Comment_ID\n",
    "    text = comment_row.Text\n",
    "    post_id = comment_row.Post_ID  # Store the post_id\n",
    "    comment = Comment(comment_id, text, post_id)\n",
    "    comments_dict[comment_id] = comment\n",
    "\n",
    "    # Assign parent comment if it exists.\n",
    "    parent_comment_id = comment_row.Parent_Comment_ID\n",
    "    if not pd.isna(parent_comment_id):\n",
    "        parent_comment = comments_dict.get(parent_comment_id)\n",
    "        if parent_comment:\n",
    "            comment.parent_comment = parent_comment\n",
    "            parent_comment.replies.append(comment)\n",
    "\n",
    "# Function to get the full thread for a given post and its comments\n",
    "def get_thread_for_post(post, comments_dict):\n",
    "    thread = f\"Title: {post.title}\\nContent: {post.content}\\n\\nComments:\\n\"\n",
    "    \n",
    "    for comment_id, comment in comments_dict.items():\n",
    "        if comment.post_id == post.post_id:\n",
    "            if comment.parent_comment is None:\n",
    "                indicator = \"T:\"  # Top-level comment indicator\n",
    "            else:\n",
    "                indicator = \"R:\"  # Reply indicator\n",
    "            # Add the comment to the thread\n",
    "            thread += f\"{indicator} Comment Text: {comment.text}\\n\"\n",
    "            \n",
    "    thread_no_urls = cleantext.replace_urls(thread, replace_with=\"<URL>\")\n",
    "    return thread_no_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "## Set up OpenAI Key\n",
    "OPENAI_API_KEY = getpass()\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-1106\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEWER EXAMPLES\n",
    "## Aspect Extraction Chain\n",
    "examples = [{\n",
    "    \"thread\": '''\n",
    "    Title: the lack of fundamentals on this team is astounding\n",
    "    Content: to me the most shocking thing is how bad this team is top to bottom when it comes to fundamentals. awful decision making, fouls, etc. we’re awful across the board. it’s one thing to not have much talent, but how could a team under Smith be so bad at the fundamentals? our older teams had great fundamentals. \n",
    "    \n",
    "    what happened? this season had really put Smith's actual coaching skills in a terrible light imo.\n",
    "    Comments: \n",
    "    T: Comment Text: It starts with coaching and buy in. Smith has lost most of the team IMO. They have no real leaders. Most guys will be gone in 2 years and they know it. A bunch of rouge players not giving a shit.\n",
    "\n",
    "    They don't trust the Smith way anymore.\n",
    "    R: Comment Text: Moulin Rouge?\n",
    "    T: Comment Text: There is nowhere else to look other than Smith. The justification to keep him around is that he should be able to field a disciplined team that gives effort and makes smart plays even if there is a talent disparity.\n",
    "\n",
    "    With how unprofessional and unprepared the team has been the last few years I’m not sure what coaching “advantage” he’s giving you anymore.\n",
    "    R: Comment Text: Jacobs has to get rid of Smith. Period\n",
    "    R: Comment Text: It's time. 2-7.\n",
    "\n",
    "    Who can this team beat in the remaining schedule? The Seattle Sounders??\n",
    "    ''',\n",
    "    \"aspects\": \"Relevant aspects are 'team performance', 'coaching and management'\"\n",
    "},\n",
    "{\n",
    "    \"thread\": '''\n",
    "    Title: Am I the only one that thinks espn hates talking abt nsc\n",
    "    Content:  I swear they talk abt everyone we beat and any other game that was on or abt to play rather than giving us any credit. From what I’ve seen they don’t want us here and they try to play down what we’ve done and have given us no credit saying that the teams we’ve beat didn’t play good or we got lucky. Let me know your thoughts\n",
    "    Comments: \n",
    "    T: Surprise! We are a small market team. It’s been this way literally forever.\n",
    "    R: How is Nashville a small market team when there are 1.5 million people in the city on any given day?\n",
    "    R: Because half of them aren’t fans/from Nashville\n",
    "    T: I think Smith went over to ESPN and bullied them all to keep the talking about us to a minimum, to help our boys stay focused and keep that hungry underdog mentality!\n",
    "    T: I just enjoy that they are talking. And I don’t think they “hate” us. Just a small market team. Hard to satisfy a national audience with a small amount of fans. They love Hany and the heart of the team.\n",
    "    T: ESPN is a known bias propagandizing network . I've switched to fox more and more this year. They giving nsc love.\n",
    "    T: People need to understand the audience isn’t the same for ESPN anymore. Its not like it used to be where every man in the world watches ESPN. A lot of the higher quality viewers are just using the internet or their phones now to look up high lights. As a result I feel these sports shows have to be more entertaining to get ratings, and as a result they just talk about the popular teams. People this is entertainment. They aren’t out here to give a fair or honest assessment.\n",
    "    R: Chicken and egg with that one. I stopped watching when ESPN became the Lebron/Tiger/Brady channel.\n",
    "    ''',\n",
    "    \"aspects\": \"Relevant aspects are 'media coverage'\"\n",
    "},\n",
    "{\n",
    "   \"thread\": '''\n",
    "    Title: LAFC Fans visiting the area looking for the best meal in Geodis Park\n",
    "    Content:  Hey all, me and a bunch of friends are visiting Nashville for a bachelor party and surprising the married man to be with tickets to the game Sunday. Seeing as we're probably gonna be hungover on death's door, what's the best meal to munch on in Geodis?\n",
    "    \n",
    "    If you can drop a name and the section in the comments I'll love you forever.\n",
    "    Comments: \n",
    "    T: I don’t care what anyone says, Daddy's Dogs down near the main entrance always slaps. Especially if you’re a couple beers deep.\n",
    "    T: If you’re dead set on eating at Geodis the BBQ nachos are my guilty pleasure.\n",
    "    T: The grilled cheese sandwiches (Ground floor, just off to the left of the entrance) absolutely fucking slap. I would get one right away as they can take a while, but god damn are they good.\n",
    "    ''',\n",
    "    \"aspects\": \"Relevant aspects are 'stadium amenities'\"\n",
    "},\n",
    "{\n",
    "    \"thread\": '''\n",
    "    Title: Make Geodis loud again!\n",
    "    Content:  This is a call to arms for all fans going to the game tonight! I know things have looked bleak as of late, but we need to remind our boys of our support, and that they shouldn’t dread playing at home for us!\n",
    "    \n",
    "    So even if we get into a big hole, let’s all band together and be leaders in the crowd tonight and cheer our team into a win! Never give up!\n",
    "    Comments: \n",
    "    T: When they jacked up the prices the crowd noise quieted down immediately. There was a precipitous drop the season after the playoff run.\n",
    "    R: Prices were going up after the playoff run anyway. The only question was whether the increase would go to NSC or to the ticket brokers.\n",
    "    T: Ridiculously expensive tickets have kept the most loyal and loudest fans home.\n",
    "    T: I was literally sitting at Wicked Weed watching the prices fall as we got closer to game time because I was having this exact conversation. It’s not the first time the tickets have hit $10. I posted a screenshot with a $10 lower bowl pair a couple weeks ago.\n",
    "    R: $10 lower bowl seats? PM me if you ever see that again lol\n",
    "    R: $20 lower bowl seats tonight. $10 in the upper bowl. Shame all the real fans are being priced out though.\n",
    "    ''',\n",
    "    \"aspects\": \"Relevant aspects are 'stadium atmosphere', 'pricing'\"\n",
    "}]\n",
    "\n",
    "prompt_template = '''\n",
    "Thread: {thread}\n",
    "{aspects}\n",
    "'''\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables = [\"thread\", \"aspects\"], template = prompt_template)\n",
    "\n",
    "final_prompt = FewShotPromptTemplate(\n",
    "    examples = examples,\n",
    "    example_prompt = example_prompt,\n",
    "    suffix = \"Thread: {thread}\\n\",\n",
    "    input_variables = [\"thread\"],\n",
    "    prefix = '''\n",
    "    I am extracting aspects from a Reddit Thread made by Nashville SC fans. Nashville SC (NSC) is a soccer team that plays at their stadium: Geodis Park in Nashville, Tennessee. Their roster consists of Joe Willis, Daniel Lovitz, Lukas MacNaughton, Nick DePuy, Jack Maher, Dax McCarty (captain), Fafà Picault, Randall Leal, Sam Surridge, Hany Mukhtar, Ethan Zubak, Teal Bunbury, Joey Skinner, Jacob Shaffelburg, Laurence Wyke, Shaq Moore, Alex Muyl, Aníbal Godoy, Ahmed Longmire, Josh Bauer, Taylor Washington, Walker Zimmerman, Luke Haakenson, Brian Anunga, Nebiyou Perry, Elliot Panicco, Sean Davis, Ben Martino, Adem Sipić, and Kemy Amiche. Their coach is Gary Smith and their general manager (GM) is Mike Jacobs. Any other player names that are given, assume they are on an opposing team.\n",
    "    For this conversational thread, please return a list of the following aspects of fan experience that are present in the thread: 'team performance' (specifically NSC, ignore discussion about other teams), 'stadium amenities' (including food and the team store), 'coaching and management', 'pricing', 'stadium atmosphere' (including comfort, safety, and crowd atmosphere), 'media coverage', and a 'miscellaneous' category. The 'miscellaneous' category should be returned if there is general conversation that is not covered by the other aspects.\n",
    "    The structure of the post will be as follows: Title is the general title made of the original post. Content is the text from the original post. Comments will be all comments on the post. Any comment labelled as \"T: Comment Text\" is a top-level comment, so use the original post and content as the context for this comment. Any comment labelled as \"R: Comment Text\" is a reply comment, so use all the comments above it until you hit a top-level comment, as well as the original post and content as the context for this comment. If the Content of a post is \"nan\", that means the post was an image. For any of these posts, just consider the post title and comments, ignoring the \"nan\" content.\n",
    "    ''')\n",
    "\n",
    "aspect_extraction_chain = LLMChain(llm = llm, prompt = final_prompt, output_key = 'aspects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment Analysis Chain\n",
    "prompt_template2 = '''\n",
    "The following text is a Reddit thread and the list of aspects extracted from that thread. The aspects will be in the format ['aspect1', 'aspect2'...]. For each aspect in the list, return a sentiment score. Do NOT create aspects of your own; only calculate scores for the aspects provided. This sentiment score should be on a continuous scale from -1 to 1, where -1 is the most negative sentiment, 1 represents the most postive sentiment, and relatively neutral sentiments fall in the range -0.3 to 0.3. Round the score to 2 decimal places. Your output should follow this format: [(Aspect1, Sentiment_Score_1), (Aspect2, Sentiment_Score_2),.....].\n",
    "Thread: {thread}\n",
    "Aspects: {aspects}\n",
    "[(Aspect1, Sentiment_Score_1), (Aspect2, Sentiment_Score_2),.....]\n",
    "'''\n",
    "\n",
    "example_prompt2 = PromptTemplate(input_variables = [\"thread\", \"aspects\"], template = prompt_template2)\n",
    "\n",
    "aspect_sentiment_chain = LLMChain(llm = llm, prompt = example_prompt2, output_key = \"Aspects_with_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full Sequential Chain\n",
    "overall_chain = SequentialChain(\n",
    "    chains = [aspect_extraction_chain, aspect_sentiment_chain],\n",
    "    input_variables = [\"thread\"],\n",
    "    output_variables = [\"thread\", \"aspects\", \"Aspects_with_sentiment\"],\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store threads\n",
    "threads = []\n",
    "for post_id, post in posts_dict.items():\n",
    "    thread = get_thread_for_post(post, comments_dict)\n",
    "    threads.append(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Fri, 05 Jan 2024 17:30:15 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-1106', 'openai-organization': 'user-ixrwocb1n6xsa7gyu9p0fvlc', 'openai-processing-ms': '168', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '60000', 'x-ratelimit-limit-tokens_usage_based': '60000', 'x-ratelimit-remaining-requests': '9944', 'x-ratelimit-remaining-tokens': '55215', 'x-ratelimit-remaining-tokens_usage_based': '55120', 'x-ratelimit-reset-requests': '7m58.682s', 'x-ratelimit-reset-tokens': '4.784s', 'x-ratelimit-reset-tokens_usage_based': '4.879s', 'x-request-id': 'b6562ec533db6ac39af320d2a99d8246', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '840d72b30cc4620e-ORD', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "## Run on threads\n",
    "for thread in threads:\n",
    "    if i % 5 == 0:\n",
    "        time.sleep(60)\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    res = overall_chain({\"thread\": thread})\n",
    "    temp = res[\"Aspects_with_sentiment\"]\n",
    "    matches = re.findall(r'\\(([^,]+), ([^)]+)\\)', temp)\n",
    "    result_list = [(match[0], float(match[1])) for match in matches]\n",
    "    output.append(result_list)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Storing results\n",
    "ex = nsc_posts.copy()\n",
    "ex['results'] = output\n",
    "\n",
    "# Replace any quotation marks and force all lowercase\n",
    "ex['cleaned_results'] = ex['results'].apply(lambda x: [(aspect.lower().replace(\"'\", \"\").replace('\"', ''), sentiment) for aspect, sentiment in x])\n",
    "\n",
    "# List of aspects we want to find\n",
    "valid_aspects = ['team performance', 'miscellaneous', 'coaching and management', 'stadium atmosphere', 'pricing', 'stadium amenities', 'media coverage']\n",
    "\n",
    "# Filter the 'cleaned_results' column based on the list of valid aspects\n",
    "junk = ex.copy()\n",
    "junk['cleaned_results'] = junk['cleaned_results'].apply(lambda x: [(aspect, sentiment) for aspect, sentiment in x if aspect not in valid_aspects])\n",
    "ex['cleaned_results'] = ex['cleaned_results'].apply(lambda x: [(aspect, sentiment) for aspect, sentiment in x if aspect in valid_aspects])\n",
    "\n",
    "# Remove any empty lists\n",
    "ex = ex[ex['cleaned_results'].apply(lambda x: len(x) > 0)]\n",
    "junk = junk[junk['cleaned_results'].apply(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miscellaneous              597\n",
      "stadium atmosphere         460\n",
      "team performance           444\n",
      "stadium amenities          305\n",
      "pricing                    276\n",
      "coaching and management    228\n",
      "media coverage             130\n",
      "Name: cleaned_results, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Distribution of aspects\n",
    "aspect_counts = ex['cleaned_results'].explode().apply(lambda x: x[0]).value_counts()\n",
    "\n",
    "# Display the aspect counts\n",
    "print(aspect_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect\n",
      "coaching and management   -0.035351\n",
      "media coverage            -0.004769\n",
      "miscellaneous              0.119280\n",
      "pricing                   -0.068333\n",
      "stadium amenities          0.128197\n",
      "stadium atmosphere         0.169196\n",
      "team performance           0.066261\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Averaging sentiment scores\n",
    "# Explode the 'cleaned_results' column to have one row for each aspect-sentiment pair\n",
    "df_exploded = ex.explode('cleaned_results')\n",
    "\n",
    "# Extract aspect and sentiment into separate columns\n",
    "df_exploded[['aspect', 'sentiment']] = pd.DataFrame(df_exploded['cleaned_results'].tolist(), index=df_exploded.index)\n",
    "\n",
    "# Calculate the average sentiment score for each aspect\n",
    "average_sentiment = df_exploded.groupby('aspect')['sentiment'].mean()\n",
    "\n",
    "# Display the average sentiment scores\n",
    "print(average_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of aspects\n",
    "aspect_counts = junk['cleaned_results'].explode().apply(lambda x: x[0]).value_counts()\n",
    "\n",
    "# Display the aspect counts\n",
    "len(aspect_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unnecessary columns (Only run 1 time!)\n",
    "ex = ex.iloc[:, 2:]\n",
    "junk = junk.iloc[:, 2:]\n",
    "ex.drop(columns = ['results'], inplace = True)\n",
    "junk.drop(columns = ['results'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "junk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing results to csv\n",
    "ex.to_csv(path_or_buf = '../ABSA_Reddit_Results/nsc_run_1_res.csv')\n",
    "junk.to_csv(path_or_buf = '../ABSA_Reddit_Results/nsc_run_1_junk.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5206b32b5f345ee646df70bcbe011cac435e70d5fa1022839ac50730ea7455ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
